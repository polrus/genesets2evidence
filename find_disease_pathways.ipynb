{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078e378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d719798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pathways(input_folder, search_df):\n",
    "    # Load search terms and associated metadata\n",
    "    records = []\n",
    "\n",
    "    # Go through each .txt file with gene sets\n",
    "    for file_path in glob.glob(os.path.join(input_folder, '*.txt')):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    continue  # malformed line\n",
    "                \n",
    "                pathway = parts[0]\n",
    "                genes = parts[2:]\n",
    "                genes_str = ', '.join([g for g in genes if g.strip()])\n",
    "\n",
    "                # Match each search term with the pathway name\n",
    "                for _, row in search_df.iterrows():\n",
    "                    term = str(row['searchTerms'])\n",
    "                    if term.lower() in pathway.lower():\n",
    "                        records.append({\n",
    "                            'pathwayName': pathway,\n",
    "                            'searchTerms': term,\n",
    "                            'genes': genes_str,\n",
    "                            'library': file_name,\n",
    "                            'diseaseId': row.get('diseaseId'),\n",
    "                            'name': row.get('name'),\n",
    "                            'therapeuticAreas': row.get('therapeuticAreas')\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame(records, columns=[\n",
    "        'pathwayName', 'searchTerms', 'genes', 'library',\n",
    "        'diseaseId', 'name', 'therapeuticAreas'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_diseases = pd.read_csv('/home/polina/genesets2evidence/disease_list/reactome_dis_terms_curated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb04b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sets = '/home/polina/genesets2evidence/gene_sets/pathways_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a9b0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_pathways = find_pathways(gene_sets, reactome_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d220aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_pathways_grouped = disease_pathways.groupby('pathwayName').agg(lambda x: ','.join(sorted(set(x)))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e82ebb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_pathways_grouped.to_csv('disease_pathways_v1.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cab381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_pathways_grouped['genes'] = disease_pathways_grouped['genes'].str.split(', ')\n",
    "disease_pathways_exploded = disease_pathways_grouped.explode('genes', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85f4ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_pathways_exploded.to_csv('target_lists/from_reactome_dis_v2_pathways_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466a30c",
   "metadata": {},
   "source": [
    "### Parse genenames to filter out non-gene targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68499ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col, explode, split, collect_set, concat_ws, lit, filter, when, concat_ws\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc9324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c07c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Target info from OT platform to parse gene names\n",
    "target_path = \"gs://open-targets-data-releases/25.03/output/target/\"\n",
    "target = spark.read.parquet(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e424c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dataframes(initial_df: DataFrame, \n",
    "                    second_df: DataFrame, \n",
    "                    initial_key_column: str, \n",
    "                    second_key_column: str,\n",
    "                    columns_to_join: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Joins two PySpark DataFrames on specified key columns.\n",
    "\n",
    "    Args:\n",
    "    initial_df (DataFrame): The initial PySpark DataFrame.\n",
    "    second_df (DataFrame): The second PySpark DataFrame to join with.\n",
    "    initial_key_column (str): The key column name in the initial DataFrame.\n",
    "    second_key_column (str): The key column name in the second DataFrame.\n",
    "    columns_to_join (list): List of column names from the second DataFrame to include in the join.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The resulting DataFrame after the join.\n",
    "    \"\"\"\n",
    "\n",
    "    # Selecting specified columns from the second DataFrame, including its key column\n",
    "    second_df_selected = second_df.select([second_key_column] + columns_to_join)\n",
    "\n",
    "    second_columns_to_join_with_alias = [\"b.\" + col for col in columns_to_join]\n",
    "\n",
    "    return initial_df.alias(\"a\")\\\n",
    "        .join(second_df_selected.alias(\"b\"), \n",
    "            on = initial_df[initial_key_column] == second_df_selected[second_key_column], \n",
    "            how='left')\\\n",
    "        .select(\"a.*\", *second_columns_to_join_with_alias)\\\n",
    "        .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f78867",
   "metadata": {},
   "outputs": [],
   "source": [
    "geneset_evidence1 = pd.read_csv('target_lists/from_reactome_dis_v1.csv', sep=',', header=0)\n",
    "geneset_evidence1_spark = spark.createDataFrame(geneset_evidence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89eaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "geneset_evidence2 = pd.read_csv('target_lists/from_reactome_dis_v2_pathways_only.csv', sep=',', header=0)\n",
    "geneset_evidence2_spark = spark.createDataFrame(geneset_evidence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "790886df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/13 11:04:43 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "target_list = [\"id\"]\n",
    "\n",
    "geneset_evidence1_genenames = join_dataframes(geneset_evidence1_spark, target, \"genes\", \"approvedSymbol\", target_list).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f32b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/13 11:04:44 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "geneset_evidence2_genenames = join_dataframes(geneset_evidence2_spark, target, \"genes\", \"approvedSymbol\", target_list).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12678a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geneset_evidence1_genenames.toPandas().to_csv('target_lists/from_reactome_dis_v1_genenames.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d23da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geneset_evidence2_genenames.toPandas().to_csv('target_lists/from_reactome_dis_v2_pathways_only_genenames.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23b759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
